# ファイルシステムのベンチマークを行う際に注意することとは?
という視点で次の論文を読みます:
https://www.fsl.cs.sunysb.edu/docs/fsbench/fsbench.pdf

# 0. 概要
* ファイルシステムやストレージシステムのベンチマークによる比較は難しい 
     * システムは色々なコンポーネントの組み合わせで成り立っているところで動く
     * システム毎に特性などが違う
* そのため単一のベンチマークでの比較は難しい
     * （汎用性があるため）様々な箇所で稼働することになり現実的なワークロードを反映するのも難しい
* この論文ではファイルシステム・ストレージシステムの106件の論文をサーベイした結果を述べる。
     * オーバーヘッドを隠蔽していたり、逆に大げさに強調しているようなものもみられる。
* 多くのベンチマークには欠点があり、システムの真の性能を発揮して比較できていないことを述べる。
     * 特筆すべき例として「slowing down read operations on ext2 by a factor of 32 resulted in only a 2–5% wall-clock slowdown in a popular compile benchmark」とあるがこの段階では何が言いたいのか良くわからない。
* とりあえずこの論文ではベンチマークをする上で今後どうするべきかを論じたい。

先に ext2 のどうこういう例が何なのかだけ確認する（もうちょっと良い文章を書いてほしい……）
12.2節でこの話が書かれている。compiler benchmarkという類のベンチマークに関して、ファイルシステムのreadのレイテンシ時間を
`2^5=32`倍遅くした（これをやるためにslowfsというものを作っている）場合とそうでない場合とで合計の実行時間に殆ど差がないことから、
ベンチマークの殆どの時間はCPUを使った計算に費やされていると結論付けているものである。

これぐらいならtimeコマンドのreal timeとcpu timeの引き算で気づけそうなものだが、それで気づけないかどうかぐらいは書いていると思うので、
とりあえず読み進める。

# 1. 序論
* ベンチマークの重要さを訥々と語っている
* この論文ではベンチマークセットの「選び方」または「作り方」に焦点をあてる
* ベンチマークを3種類に分類した
      * マクロベンチマーク: 現実のワークロードを使ったベンチマーク 7章で扱う
      * トレース再現（trace replays): 操作をreplayするらしいがこの段階では良くわからない 8章で扱う
      * マイクロベンチマーク: 数個の操作に絞って操作毎の細かなパフォーマンスをみるためのベンチマーク 9章で扱う

# 2. サーベイした論文について
SOSP, OSDI, FAST, USENIXが良い論文を通しているだろうということで、この中から選んだという話。
サーベイした論文については、この論文のReferencesにそれと分かるようにアスタリスク付きで書いている。

# 3. システムをベンチする時に考慮した方が良いこと
ものすごく簡単に言うと何をベンチマークとして実施したのかということや、選択肢のある中なら何故そうしたのかということをちゃんと書きましょうという話。

## 3.1 パフォーマンス比較の重要な三要素 <system, configuration, benchmark type>
* system
     * いわゆる実験に使うマシンとかソフトウェアとか
* configuration
     * systemを決めた後に変更可能な要素で、ベンチマーク対象が直接的に持つ mode とか parameter とかこの辺だと思う
     * 直接的という表現にしているのは、3.2 で考えるenvironmentと区別したいから（これはyuezatoが勝手に足した表現)
* benchmark type     
     * Macrobenchmark: 複数の命令を使ってsystemの全体的な性能を測る
     * Microbenchmark: １つか２つの命令に絞って個別または局所的な性能を測る
     * Trace-based: いまいち雰囲気がつかめない [あとでかきなおす]
     * どのベンチマークでも大事なことは次の5つだと考えている
          * CPUとIOのどっちがどれぐらい絡んでくるか・割合が明らか
          * ベンチマーク自身が時間計測をする場合はそれが正確なものであること（測定すると予告していることを正しく測っているという意味においてだと思う）
          * スケールすること; 性能に関して謎の頭打ちなどが生じることなく、ハードウェアやソフトウェアが進歩しても絶えず使えるようなもの
          * マルチスレッド; ことファイルシステムは普通マルチスレッドなプログラムで使役されるので、現実的な用途も調べられなくてはならない
          * 理解可能なものでありかつ再現可能であること

## 3.2 Choosing The Benchmarking Environment
systemというかmachineかも知れないが、ベンチマークに影響を及ぼす様々な環境について考慮しなくてはならない。
少なくともresultはどの環境で行われたベンチマークのresultかという環境とのセットでも提出される必要がある。
環境というのは
* キャッシュの状態（CPUキャッシュに限らずファイルシステムのキャシュとかも諸々）
     * キャッシュが効く（warm)にしたければ連続して走らせて外れ値を捨てたりするとか
     * キャッシュが効かない(cold)にしたければ逐一でrebootしろとか
     * そういうことが書いてます
* HDDなどのZCAVのon/off
     * 最近のHDDは、内周よりも外周の面積のほうが大きい分だけセクタサイズが増え、外周では1セクタ間の移動時間が短くなり云々という話がある
     * これを極力抑えたければ`[Ellard and Seltzer 2003b]`のような工夫もできるが、現実的ではないので、この論文ではpartitionの位置を明記して再現性を出す程度で留める
     * `[Ellard and Seltzer 2003b]` はこれ https://dash.harvard.edu/handle/1/2799040 Tagged Command Queueなどについても書いており一読の価値あり
* File Systemのaging（fragmentationの度合いとか)
     * 現実に近い環境を作るならファイルシステムをマウントして「長い時間をかけて」agingを行うべきだが、本当に長い時間がかかるのでagingは大変である
     * 大変だが考慮するべき
     * Agingに関してはFAST'19でこういう論文があったので一読 https://www.usenix.org/conference/hotstorage19/presentation/conway
* ベンチマーク中に動いている他のプロセスの有無（現実的なシステムでは他のプロセスも動いてるのでそこも考えなくてはならない)
     * 現実的には他のプロセスも動かしたいが再現性などが薄れてしまう
     * 代わりにベンチマークをマルチスレッド化すれば現実味は出てくるだろう [いまいち意味が分からないので後で書き直す]
        
## 3.3 Running the Benchmarks
以下が大事
* 比較環境で同一のrunが行われること
* 十分に多い回数実験すること
* 各runは十分に長い時間であること
* 人手だとミスするのでautomationすること 11章で述べる
     * やったと主張している通りのベンチマークが実はできていなくて正しい結果が出ていないということがよくあるので賢くなれよという話だと思う
     
## 3.4 結果を記す時には
* 標準偏差よりも信頼区間をもとにした方が良い
     * The standard deviation is a measure of the amount of variation between runs. 
     * 標準偏差は実際に採取したrunにおけるデータの散らばりをあらわしている
     * The half-width of the confidence interval describes how far the true value may be from the captured mean with a given degree of confidence (e.g., 95%)
     * 信頼区間は真値と採集したデータの離れ具合に焦点を当てているのでこの２つは異なる
* 1ベンチマークに対して30runより少ない場合は信頼区間の計算に正規分布を使うべきではない。これは中央値定理がサンプル数の少ない場合には成立しないからで、t分布を用いることを考えるべきである。
* という統計入門的なことが書いてある。（が、これを考慮してベンチマークしている論文は殆どみたことないな）

# 7 Macrobenchmarks
* サーベイした論文中に現れた各マクロベンチマークの利点と欠点について述べる。
* マクロベンチマークとは、現実世界のワークロードを反映する複数種類の命令からなるベンチマークである。
* ファイルシステムのベンチマークを作る組織がない
    * データベースには TPC: http://www.tpc.org/information/benchmarks.asp
    * ストレージには SPC: https://spcresults.org/
* 良く使われているからという理由だけでベンチ項目として採用されており、そのマクロベンチマークが何をして提唱するシステムとどう関係するかが書かれていないことが多い
* Postmark, コンパイルベンチ、 Andrew, TPC, SPEC, SPC, NetNewsなどを取り扱っている
    * このノートでは全部書くと大変なのでいくつかだけ抜粋して書く
    
## Postmark
https://openbenchmarking.org/test/pts/postmark
* メーラーの挙動を模倣しており、比較的小さなサイズのファイルを数百個作って読み書きするというもの
* デフォルトパラメータでのワークロードでは数秒で終わるため、FSのパフォーマンスを測るという意味ではよろしくない
* マルチスレッドをサポートしていないのでマクロベンチとしては現代では微妙
* 他にも諸々あるが、それらの問題を改善すれば良いものになると思う

## コンパイルベンチマーク
* 大きめのプロジェクトをコンパイルするというもの
    * SSHやらKernelやらEmacsやら[Am-utils](https://www.am-utils.org/)やら
* CPUの負荷が高く、FSの問題点が隠れてしまう恐れがある
* 再現性が難しい
    * ビルドツールがユーザの環境毎に違うので
* どういった操作の偏りになるのか十分に説明されなければならない
    * 例えばSSHとAm-utilsではread/write比が全然違うといったことは相当詳細に調べないと分からない
* 今日日の軽量仮想化技術を使ってビルド環境を提供するというぐらいしないとまともな比較にはならないと思われる。

## Andrew Filesystem Benchmark
* ユーザのファイルシステム上での活動を意図している（らしいがお気持ち程度かもしれない)
* プログラムのコンパイルのようなこともやるのでコンパイルベンチマークの欠点を引き継ぐ
* データが全体的に小さく、ページキャッシュなどのバッファなどにのるため、ディスクが使われない可能性が高い
* 使わない方がいいだろう

## TPC
* データベース用のベンチで色々ご種類がある
* データベースを使ったテストなのでFSのベンチとして導入するのは難しい
     * 代替案としては、tracebasedに切り替えるという手がある。

## SPC
* SPC-1と呼ばれる小規模データによるベンチマークとSPC-2と呼ばれる1MB弱までのデータによるシーケンシャルアクセスパターンよりのベンチマーク、2つがある。
     * https://www.fujitsu.com/jp/products/computing/storage/disk/eternus-dx/spc/
* 良さそう、試したい！
     * 有料

# 8 Replaying Traces
操作のログを残しておいて、それを再生することでワークロードを走らせる方法。
いくつか考慮しなければならない点がある。

***実際に動かしてみたいけれど、何が使えるツールなのかが分からない***

## capture方法
システムコールをキャプチャするのか、VFS命令をキャプチャするのか、ドライバレベルでキャプチャするのか、どれよという話。

システムコールをキャプチャする方法は比較的楽だが、メモリマップドであれば拾えなくなったりする。
VFS命令だと移植性が薄くなる、など。

## replay方法
* captureした方法に依存するが、トレースがreplay可能であることの検査は必要
* ファイルシステムの状況再現も必要（操作対象のファイルが存在していて、ファイルサイズがいくつでというような話）
      * agingも必要になるかもしれない
* タイミング問題がややこしい
      * トレースと同じタイミングで実行するべき派（[Buttress](https://www.usenix.org/conference/fast-04/buttress-toolkit-flexible-and-high-fidelity-io-benchmarking)の作者など）もいる; トレースを作った側のマシンの方がリプレイ側より速かったらどうするねんとう話もある
      * リプレイする側のマシンで最速で実行すれば良い派もいる（サーベイした範囲ではこっちの方が多数）
* この辺の話はちゃんとやろうとするだけで１つの論文になってしまう
     * https://static.usenix.org/events/fast05/tech/full_papers/joukov/joukov.pdf

## trace availiability（再現性に関する話)
* トレースは巨大になりがちなので保持するのがとにかく大変である
* いまはここに集まる感じになっている: http://iotta.snia.org/traces/


# 9 Microbenchmarks
マイクロベンチマークはマクロベンチマークの説明のために使われるというのが良い使われ方だと書いている。
マクロベンチマークの結果がなぜそうなるのか、どこにオーバーヘッドがあるかを推理するために、より小さな問題への分解先としてマイクロベンチが使われるのは良い。

マイクロベンチマークは局所的な性質が出るので（操作としても環境としても）、汎用使用パフォーマンスとしての比較をするなら、相当の注意を払わなければならない。
マクロベンチマークだと局所性が償却されるかもしれないが、それすら行われないので。

## Bonnie, Bonnie++
単一ファイル上でシーケンシャルwrite, シーケンシャルread, ランダムシークするマイクロベンチマーク。
Bonnie++はC++で書かれていていくつか機能が追加されている。
パラメータとしてはファイルサイズしか変更できない。

## Sprite LFS
LargeFileとSmallFileの二つのベンチマークがある。

LargeFileについては今日のファイルサイズとしては小さい100MBで固定であるなどの様々な問題がある。

SmallFileは1KiBのファイルを作り、読み込んだ後に削除するもの。存在は確からしいがソースコードがなかったということで、
論文の著者らが `LFS-{SH1, SH2, SH3, C, PL}`という3バージョンの[実装を行った](https://www.fsl.cs.sunysb.edu/docs/fsbench/onlinedata.html)が、殆どキャッシュが効いてDiskアクセスはなかったということ。

## 好き勝手書かれたベンチマーク
再現性が低かったり、ベンチマーク自身がぶっ壊れている可能性があるのでやらない方が良い。
それやるぐらいなら、Linuxのコマンドを使った方がマシかも知れない。

# 10 Configurable Workload Generator
* IntelのIometer
     * 優秀なのでこれで済むなら使った方が良い
* HPのButtress
     * Iometerより細かなconfigurationが出来るがその分面倒くさい
     * 今はどうか知らないがHPから貰ってこないと使えない
* SunのFileBench
     * スクリプト言語でワークロードを書くタイプ
     * マルチプロセス・マルチスレッド指向の記述ができる 
     * warm-up/cool-down phaseで細かい時間計測が可能
     * 使えるなら使った方が良い

# 11 Benchmariking Automation
ベンチマークのメタ操作（何回も実行したりパラメータを変えながら実行したりデータを統計的に処理したりするような）を自動化するツールとして、
[Auto-pilot](https://www.filesystems.org/project-autopilot.html)を作っているので皆使ってねという話

# 12 Experimental Evaluations
ベンチマークに潜む落とし穴をわかりやすく明らかにしたい。そこでSlowfsと呼ばれるext2のvariantを使う。

Slowfsはパラメータ`N`を用いて、指定したVFS層での操作の実行に掛かった時間分だけ、パラメータを使って遅くするというもの。
```
(1) start = getcc() [get current time in CPU cycles];
(2) calls the original function for the operation;
(3) now = getcc();
(4) goal = now + ((now − start) * 2N) − (now − start); and 
(5) while (getcc() ≤ goal) { schedule() }.
```

## コンパイルベンチでのオーバーヘッド隠蔽
OpenSSHのver3.7と3.9で、configureとmakeのそれぞれについて、Slowfsのパラメータを大きく（大きいほど遅らせが多くなる）して実験した。
実験の結果としては、パラメータが小さい（遅らせが0)の場合と大きい場合とで実行時間に差が（殆ど）なかった。
すなわち、もともとFSの操作はCPUで計算に使っている時間に比べるとかなり小さいものであったことが分かる。

## Postmarkでの実行時間内訳
三種類のconfigurationで実験。
Read, Writeでの遅くなる度合いの違いをみてはじめて、configurationを変えただけでは見えない実際の操作の占める割合が分かる。
